.. |br| raw:: html

  <br/>
  
References
==========

**Summary:** This a is non-exhaustive list of references for this component.

|

.. contents:: **Table of Contents**

|

Model Compression
-----------------

References
^^^^^^^^^^

**2013**

- `Restructuring of Deep Neural Network Acoustic Models with Singular Value Decomposition <https://www.microsoft.com/en-us/research/wp-content/uploads/2013/01/svd_v2.pdf>`_

**2015**

- `Neurons vs Weights Pruning in Artificial Neural Networks <http://journals.rta.lv/index.php/ETR/article/view/166>`_

**2016**

- `Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations <https://arxiv.org/pdf/1609.07061.pdf>`_
- `XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks <https://arxiv.org/pdf/1603.05279.pdf>`_
- `Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <https://arxiv.org/pdf/1510.00149.pdf>`_
- `EIE: Efficient Inference Engine on Compressed Deep Neural Network <https://arxiv.org/pdf/1602.01528.pdf>`_
- `Dynamic Network Surgery for Efficient DNNs <https://arxiv.org/pdf/1608.04493.pdf>`_
- `SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size <https://arxiv.org/pdf/1602.07360.pdf>`_
- `Learning Structured Sparsity in Deep Neural Networks <https://arxiv.org/pdf/1608.03665.pdf>`_

**2017**

- `Soft Weight-Sharing for Neural Network Compression <https://arxiv.org/pdf/1702.04008.pdf>`_
- `Variational Dropout Sparsifies Deep Neural Networks <https://arxiv.org/pdf/1701.05369.pdf>`_
- `Structured Bayesian Pruning via Log-Normal Multiplicative Noise <https://proceedings.neurips.cc/paper/2017/file/dab49080d80c724aad5ebf158d63df41-Paper.pdf>`_
- `Bayesian Compression for Deep Learning <http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning.pdf>`_
- `A Survey of Model Compression and Acceleration for Deep Neural Networks <https://arxiv.org/pdf/1710.09282.pdf>`_
- `ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression <https://arxiv.org/pdf/1707.06342.pdf>`_
- `To prune, or not to prune: exploring the efficacy of pruning for model compression <https://arxiv.org/pdf/1710.01878.pdf>`_

**2018**

- `Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights <https://arxiv.org/pdf/1801.06519.pdf>`_
- `Recent Advances in Efficient Computation of Deep Convolutional Neural Networks <https://arxiv.org/pdf/1802.00939.pdf>`_
- `Bayesian Compression for Natural Language Processing <https://arxiv.org/pdf/1810.10927.pdf>`_

**2019**

- `The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks <https://arxiv.org/pdf/1803.03635.pdf>`_
- `Stabilizing the Lottery Ticket Hypothesis <https://arxiv.org/pdf/1903.01611.pdf>`_
- `Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask <https://arxiv.org/pdf/1905.01067.pdf>`_
- `Weight Agnostic Neural Networks <https://arxiv.org/pdf/1906.04358.pdf>`_
- `The State of Sparsity in Deep Neural Networks <https://arxiv.org/pdf/1902.09574.pdf>`_

**2020**

- `A Survey on Deep Neural Network Compression: Challenges, Overview, and Solutions <https://arxiv.org/pdf/2010.03954.pdf>`_
- `An Overview of Neural Network Compression <https://arxiv.org/pdf/2006.03669.pdf>`_

**2021**

- `Pruning and Quantization for Deep Neural Network Acceleration: A Survey <https://arxiv.org/pdf/2101.09671.pdf>`_
- `Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better <https://arxiv.org/pdf/2106.08962.pdf>`_

NAS
---

References
^^^^^^^^^^

See `NAS <https://github.com/GUT-AI/nas/blob/master/references/README.rst>`_.

Compressed Feature Extraction (i.e. Compression of Representation Learning Models)
----------------------------------------------------------------------------------

Note: `Representation Learning Models (RLMs) <https://github.com/GUT-AI/gut-ai/blob/master/model_zoos/README.rst#representation-learning-models-rlms>`_ can be either pretrained or not, but training (or reuse) takes place during the *stage of Preprocessing*. RLMs can be in the NLP, Computer Vision or other domain.

See also `Awesome Efficient PLM Papers <https://github.com/TobiasLee/Awesome-Efficient-PLM>`_.

References
^^^^^^^^^^

**2021**

- `NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search <https://arxiv.org/pdf/2105.14444.pdf>`_
- `Towards Efficient Post-training Quantization of Pre-trained Language Models <https://arxiv.org/pdf/2109.15082.pdf>`_
- `Compression of Generative Pre-trained Language Models via Quantization <https://arxiv.org/pdf/2203.10705.pdf>`_
- `Synergistic Self-supervised and Quantization Learning <https://arxiv.org/pdf/2207.05432.pdf>`_
